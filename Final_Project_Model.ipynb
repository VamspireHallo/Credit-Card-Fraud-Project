{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdjsQnAYUm3A"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and environment\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, auc\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Cell 2: Helper plotting functions\n",
        "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_scores, title=\"ROC Curve\"):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr,tpr):.4f}\")\n",
        "    plt.plot([0,1],[0,1],'--', color='grey')\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title(title)\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "def classification_metrics(y_true, y_pred, y_scores=None):\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "    if y_scores is not None:\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_true, y_scores))\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(os.path.join(path, \"creditcard.csv\"))\n",
        "df.head()\n",
        "# Cell 4: Quick EDA\n",
        "print(df['Class'].value_counts())\n",
        "print(df.describe().T)\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title(\"Class distribution (0=legit,1=fraud)\")\n",
        "plt.show()\n",
        "\n",
        "# Distribution of Amount (skewed)\n",
        "sns.histplot(df['Amount'], bins=50, kde=True)\n",
        "plt.title(\"Transaction Amount distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Cell 5: Feature engineering & preprocessing\n",
        "# Create 'Hour' from 'Time' (seconds elapsed)\n",
        "df['Hour'] = (df['Time'] // 3600) % 24\n",
        "# Log1p transform for Amount\n",
        "df['Amount_log'] = np.log1p(df['Amount'])\n",
        "\n",
        "# Drop original Time/Amount if desired, keep engineered\n",
        "df_model = df.drop(['Time','Amount'], axis=1)\n",
        "df_model.head()\n",
        "\n",
        "# Cell 6: Train/test split\n",
        "X = df_model.drop('Class', axis=1)\n",
        "y = df_model['Class']\n",
        "\n",
        "# Standardize numerical columns (V1..V28, Hour, Amount_log)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "# Save scaler\n",
        "joblib.dump(scaler, \"scaler_creditcard.joblib\")\n",
        "\n",
        "# Cell 7: Handle imbalance options\n",
        "# Option A: Use class_weight in the model (we will use for RandomForest)\n",
        "# Option B: Use SMOTE for oversampling - show example\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "print(\"After SMOTE:\", np.bincount(y_train_sm))\n",
        "\n",
        "# Cell 8: MODEL 1 - XGBoost (or RandomForest fallback)\n",
        "# We'll try XGBoost if available; else RandomForest\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    model_xgb = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
        "                              use_label_encoder=False, eval_metric='auc', scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),\n",
        "                              random_state=42)\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "    preds_xgb = model_xgb.predict(X_test)\n",
        "    proba_xgb = model_xgb.predict_proba(X_test)[:,1]\n",
        "    print(\"XGBoost trained\")\n",
        "    classification_metrics(y_test, preds_xgb, proba_xgb)\n",
        "    plot_confusion_matrix(y_test, preds_xgb, \"XGBoost Confusion Matrix\")\n",
        "    plot_roc(y_test, proba_xgb, \"XGBoost ROC\")\n",
        "    joblib.dump(model_xgb, \"xgb_creditcard.pkl\")\n",
        "except Exception as e:\n",
        "    print(\"XGBoost not available or failed, falling back to RandomForest:\", e)\n",
        "    rf = RandomForestClassifier(n_estimators=200, class_weight='balanced_subsample', random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    preds_rf = rf.predict(X_test)\n",
        "    proba_rf = rf.predict_proba(X_test)[:,1]\n",
        "    classification_metrics(y_test, preds_rf, proba_rf)\n",
        "    plot_confusion_matrix(y_test, preds_rf, \"RandomForest Confusion Matrix\")\n",
        "    plot_roc(y_test, proba_rf, \"RandomForest ROC\")\n",
        "    joblib.dump(rf, \"rf_creditcard.pkl\")\n",
        "\n",
        "# Cell 9: Feature importance (if RF or XGB)\n",
        "try:\n",
        "    imp = model_xgb.get_booster().get_score(importance_type='weight')\n",
        "    # Convert to pandas\n",
        "    imp_df = pd.DataFrame(list(imp.items()), columns=['feature','importance']).sort_values('importance',ascending=False)\n",
        "    display(imp_df.head(10))\n",
        "except:\n",
        "    try:\n",
        "        imp = rf.feature_importances_\n",
        "        feat_names = X.columns\n",
        "        imp_df = pd.DataFrame({'feature':feat_names, 'importance':imp}).sort_values('importance',ascending=False)\n",
        "        display(imp_df.head(10))\n",
        "    except:\n",
        "        print(\"Feature importance not available.\")\n",
        "\n",
        "# Cell 10: MODEL 2 - Autoencoder (Unsupervised anomaly detection)\n",
        "# Train autoencoder only on legitimate transactions from training set\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential, callbacks\n",
        "\n",
        "# Separate legitimate only for training\n",
        "X_train_legit = X_train[y_train==0]\n",
        "print(\"Legit-only training shape:\", X_train_legit.shape)\n",
        "\n",
        "input_dim = X_train_legit.shape[1]\n",
        "ae = Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(input_dim, activation='linear')  # linear for reconstruction\n",
        "])\n",
        "ae.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = ae.fit(X_train_legit, X_train_legit, epochs=100, batch_size=2048, validation_split=0.1, callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# Save model\n",
        "ae.save(\"autoencoder_creditcard.h5\")\n",
        "\n",
        "# Cell 11: Use autoencoder to compute reconstruction error, set threshold\n",
        "# Compute MSE for test set, then pick threshold (e.g., 99th percentile of train legit errors)\n",
        "recon_train = ae.predict(X_train_legit)\n",
        "train_mse = np.mean(np.power(X_train_legit - recon_train, 2), axis=1)\n",
        "threshold = np.percentile(train_mse, 99)  # tuneable\n",
        "print(\"Chosen threshold:\", threshold)\n",
        "\n",
        "# Evaluate on test set\n",
        "recon_test = ae.predict(X_test)\n",
        "test_mse = np.mean(np.power(X_test - recon_test, 2), axis=1)\n",
        "y_pred_ae = (test_mse > threshold).astype(int)\n",
        "\n",
        "classification_metrics(y_test, y_pred_ae)\n",
        "plot_confusion_matrix(y_test, y_pred_ae, \"Autoencoder Confusion Matrix\")\n",
        "# ROC for AE (use MSE as score)\n",
        "plot_roc(y_test, test_mse, \"Autoencoder ROC (higher score => anomaly)\")\n",
        "\n",
        "# Cell 12: Compare model performance summary\n",
        "# Load saved models and display a small summary\n",
        "import json\n",
        "results = {}\n",
        "# XGB/RF results\n",
        "try:\n",
        "    results['xgb'] = {\n",
        "        'roc_auc': float(roc_auc_score(y_test, proba_xgb)),\n",
        "        'precision_recall': None\n",
        "    }\n",
        "except:\n",
        "    results['rf'] = {\n",
        "        'roc_auc': float(roc_auc_score(y_test, proba_rf)),\n",
        "        'precision_recall': None\n",
        "    }\n",
        "results['autoencoder'] = {\n",
        "    'roc_auc': float(roc_auc_score(y_test, test_mse))\n",
        "}\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Cell 13: Save scaler & a function to preprocess single transaction for demo\n",
        "joblib.dump(scaler, \"scaler_creditcard.joblib\")\n",
        "# Prepare a convenience function: expects raw features similar to original dataset (V1..V28, Amount, Time)\n",
        "def preprocess_transaction(row_dict):\n",
        "    # row_dict: dict with keys 'V1'...'V28', 'Amount', 'Time'\n",
        "    tmp = {}\n",
        "    tmp['Hour'] = (row_dict['Time'] // 3600) % 24\n",
        "    tmp['Amount_log'] = np.log1p(row_dict['Amount'])\n",
        "    feat_order = [f'V{i}' for i in range(1,29)] + ['Hour','Amount_log']\n",
        "    arr = np.array([row_dict[f] for f in feat_order], dtype=float).reshape(1,-1)\n",
        "    arr_scaled = scaler.transform(arr)\n",
        "    return arr_scaled\n",
        "\n",
        "# Cell 14: Load secondary dataset (IEEE-CIS) - minimal alignment & testing\n",
        "# This dataset is large and complex; here we select a subset of numeric features for quick compatibility testing.\n",
        "# Ensure you have train_transaction.csv and train_identity.csv in the working directory\n",
        "if os.path.exists(\"train_transaction.csv\"):\n",
        "    trans = pd.read_csv(\"train_transaction.csv\", nrows=200000)  # limit for speed; remove nrows for full\n",
        "    print(\"IEEE-CIS sample shape:\", trans.shape)\n",
        "    # Basic selection: 'TransactionAmt' ~ Amount; 'TransactionDT' ~ Time; plus a few numeric columns if present\n",
        "    # We'll try to extract numeric-only features and scale them to the same dimension, but note: features are different.\n",
        "    # For true evaluation, implement feature mapping and feature-engineering so the features match model input.\n",
        "else:\n",
        "    print(\"No IEEE-CIS transaction file found. Download from Kaggle and place train_transaction.csv here for domain generalization tests.\")\n",
        "\n",
        "# Cell 15: Quick note & instructions (save)\n",
        "print(\"Notebook progress saved. Next: tune hyperparameters, perform cross-validation, produce final plots, prepare model packaging (Gradio app and requirements.txt).\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
